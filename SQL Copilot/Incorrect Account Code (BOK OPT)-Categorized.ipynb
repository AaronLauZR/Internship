{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud\n",
    "#!pip uninstall scikit-learn\n",
    "#!pip install gensim\n",
    "#!pip install scikit-learn\n",
    "#!pip install fdb\n",
    "#!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5ffbf",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24652d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SQL\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import fdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import io\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Model building library \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Embedding\n",
    "# # Imbalanced \n",
    "# from imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7137a4",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostname = \"localhost\"\n",
    "database_path = \"C:/eStream/SQLAccounting/DB/BOK OPT SDN BHD.FDB\"\n",
    "username = \"SYSDBA\"\n",
    "password = \"masterkey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba12ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection\n",
    "connection = fdb.connect(\n",
    "    host=hostname,\n",
    "    database=database_path,\n",
    "    user=username,\n",
    "    password=password,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Cursor object that operates in the context of Connection con:\n",
    "cur = connection.cursor()\n",
    "\n",
    "# Execute the SELECT statement:\n",
    "cur.execute(\"select * from GL_CBDTL\")\n",
    "\n",
    "# Fetch all records\n",
    "records = cur.fetchall()\n",
    "\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "\n",
    "cbdt = pd.DataFrame(records, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Cursor object that operates in the context of Connection con:\n",
    "cur = connection.cursor()\n",
    "\n",
    "# Execute the SELECT statement:\n",
    "cur.execute(\"select * from GL_CB\")\n",
    "\n",
    "# Fetch all records\n",
    "records = cur.fetchall()\n",
    "\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "\n",
    "\n",
    "cb = pd.DataFrame(records, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Cursor object that operates in the context of Connection con:\n",
    "cur = connection.cursor()\n",
    "\n",
    "# Execute the SELECT statement:\n",
    "cur.execute(\"SELECT t1.CODE, \\\n",
    "                    t1.DESCRIPTION AS CODEDESCRIPTION, \\\n",
    "                    t2.DESCRIPTION AS CATEGORY \\\n",
    "            FROM GL_ACC t1 \\\n",
    "            LEFT JOIN GL_ACC t2 ON t1.PARENT = t2.DOCKEY \\\n",
    "            WHERE t1.PARENT != -1;\")\n",
    "\n",
    "# Fetch all records\n",
    "records = cur.fetchall()\n",
    "\n",
    "column_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "\n",
    "connection.close()\n",
    "\n",
    "acc = pd.DataFrame(records, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f334ae6",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0010781",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84774f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbdt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397fff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94875cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null value\n",
    "cbdt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each CODE\n",
    "category_counts = cbdt['CODE'].value_counts()\n",
    "\n",
    "# Display unique values and their counts\n",
    "print(\"Category Counts:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Display unique values and their counts\n",
    "display(category_counts.reset_index().rename(columns={'index': 'Category', 'Category': 'Count'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbe014",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter cashbook column needed to use\n",
    "selected_columns = [\"DOCKEY\", \"DOCTYPE\", \"DESCRIPTION\"]\n",
    "filtered_cb = cb[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb94f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cashbook and cashbook detail by the Dockey\n",
    "merged_cb = pd.merge(filtered_cb, cbdt, on='DOCKEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40778a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23022434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the doctype to remove OR\n",
    "merged_cb_categorized = pd.merge(merged_cb, acc, on='CODE', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06419179",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cb_categorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values from the specified column\n",
    "categories = merged_cb_categorized['CATEGORY'].unique()\n",
    "\n",
    "# Create a dictionary to store individual dataframes\n",
    "dfs = {}\n",
    "\n",
    "# Iterate through unique values and create dataframes\n",
    "for category in categories:\n",
    "    # Create a new dataframe for each unique value\n",
    "    new_df = merged_cb_categorized[merged_cb_categorized['CATEGORY'] == category].copy()\n",
    "    \n",
    "    # Store the new dataframe in the dictionary\n",
    "    dfs[f'{category}_df'] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e44de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dfs.items():\n",
    "    print(f\"\\n{key}:\\n{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in dfs.items():\n",
    "#     value_counts = value['CODE'].value_counts()\n",
    "#     values_to_keep = value_counts[value_counts > 5].index\n",
    "#     dfs[key] = dfs[key][dfs[key]['CODE'].isin(values_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bfb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to normalize word by removing punctuation, numbers and convert them into same case\n",
    "def remove_punctuation_and_numbers(text):\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text_no_punct = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text_no_nums = re.sub(r'\\d', ' ', text_no_punct)\n",
    "    \n",
    "    # lower text\n",
    "    text_lower = text_no_nums.lower()\n",
    "    \n",
    "    return text_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfs.items():\n",
    "    dfs[key]['Cleaned_Description_x'] = df['DESCRIPTION_x'].apply(remove_punctuation_and_numbers)\n",
    "    dfs[key]['Cleaned_Description_y'] = df['DESCRIPTION_y'].apply(remove_punctuation_and_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfs.items():\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop DataFrames with no records\n",
    "dfs = {key: df.dropna(subset='Cleaned_Description_y') for key, df in dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76732e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_name(row):\n",
    "    # Remove the content of sentence2 from sentence1 \n",
    "    final_description = row['Cleaned_Description_y'].replace(row['Cleaned_Description_x'], \" \")\n",
    "    return final_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_cb['final_description'] = merged_cb.apply(remove_name, axis=1)\n",
    "for key, df in dfs.items():\n",
    "    result_columns = df.apply(remove_name, axis=1)\n",
    "    df['final_description'] = result_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc45968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b400619",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfs.items():\n",
    "    df['Lemmatized_Description'] = df['final_description'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbddebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35363ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfs.items():\n",
    "    df['NoStopWord_Description'] = df['Lemmatized_Description'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9263157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop DataFrames with no records\n",
    "keys_to_drop = []\n",
    "\n",
    "dfs = {key: df[df['NoStopWord_Description'] != ''] for key, df in dfs.items()}\n",
    "for key, df in dfs.items():\n",
    "    if df.empty:\n",
    "        # DataFrame is empty, add key to the list for dropping\n",
    "        keys_to_drop.append(key)\n",
    "        print(key)\n",
    "        \n",
    "for key in keys_to_drop:\n",
    "    dfs.pop(key, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e58c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfs.items():\n",
    "    print(f\"Number of records in {key}: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61632d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show top 50 highest frequency words\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "descriptions = []\n",
    "vocabularies = []\n",
    "frequencey_dists = []\n",
    "for key, df in dfs.items():\n",
    "    description = df.NoStopWord_Description.str.cat(sep=' ')\n",
    "    descriptions.append(description)\n",
    "    #function to split text into word\n",
    "    tokens = word_tokenize(description)\n",
    "    vocabularies.append(set(tokens))\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f613f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store keys of DataFrames to be dropped\n",
    "keys_to_drop = []\n",
    "\n",
    "# Check each DataFrame in the dictionary\n",
    "for key, df in dfs.items():\n",
    "    if len(df) < 100:\n",
    "        # DataFrame has fewer than 100 records, add key to the list for dropping\n",
    "        keys_to_drop.append(key)\n",
    "\n",
    "# Drop DataFrames with fewer than 100 records\n",
    "for key in keys_to_drop:\n",
    "    dfs.pop(key, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba2d7f",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61802873",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_models = []\n",
    "for key, df in dfs.items():\n",
    "    lemmatized_descriptions = [description.split() for description in df['NoStopWord_Description']]\n",
    "    print(key)\n",
    "    w2v_models.append(Word2Vec(lemmatized_descriptions, vector_size = 100, window =5, min_count = 5, workers = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98beaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for w2v_model in w2v_models:\n",
    "    vocab.append(list(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f72a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = []\n",
    "max_lengths = []\n",
    "vocab_sizes = []\n",
    "for key, df in dfs.items():\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(df['NoStopWord_Description'])\n",
    "    tokenizers.append(tokenizer)\n",
    "\n",
    "\n",
    "    # pad sequences \n",
    "    max_lengths.append(max([len(s.split()) for s in df['NoStopWord_Description']]))\n",
    "\n",
    "    # define vocabulary sizes\n",
    "    vocab_sizes.append(len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5486485",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_json = tokenizer.to_json()\n",
    "# with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "#     f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bbdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "ys = []\n",
    "for key, df in dfs.items():\n",
    "    Xs.append(df['NoStopWord_Description'])\n",
    "    ys.append(df['CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = []\n",
    "X_pads = []\n",
    "\n",
    "for i in range(len(Xs)):\n",
    "    X_token = tokenizers[i].texts_to_sequences(Xs[i])\n",
    "    X_tokens.append(X_token)\n",
    "    X_pads.append(pad_sequences(X_token, maxlen = max_lengths[i], padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc73fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_dicts = []\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word_vec_dict = {}\n",
    "    for word in vocab[i]:\n",
    "        word_vec_dict[word] = w2v_models[i].wv.get_vector(word)\n",
    "    word_vec_dicts.append(word_vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97025db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenizers))\n",
    "print(len(word_vec_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_matrixs = []\n",
    "\n",
    "for i in range(len(tokenizers)):\n",
    "    embed_matrix = np.zeros(shape = (vocab_sizes[i],100))\n",
    "    for word, y in tokenizers[i].word_index.items():\n",
    "        embed_vector = word_vec_dicts[i].get(word)\n",
    "        if embed_vector is not None:\n",
    "            embed_matrix[y] = embed_vector\n",
    "    embed_matrixs.append(embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummies = []\n",
    "for y in ys:\n",
    "    y_dummies.append(pd.get_dummies(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d37631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea54595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# smote = SMOTE(sampling_strategy='minority', k_neighbors=5)\n",
    "\n",
    "# x_sm, y_sm = smote.fit_resample(X_pad, y_dummies.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d66980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ys_list = []\n",
    "for y in ys:\n",
    "    str_ys_list.append([str(str_y) for str_y in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3868f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = []\n",
    "for str_ys in str_ys_list:\n",
    "    num_labels.append(len(np.unique(str_ys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "y_trains = []\n",
    "y_tests = []\n",
    "\n",
    "for i in range(len(X_pads)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pads[i], y_dummies[i], test_size=0.2, random_state=42)\n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "    y_trains.append(y_train)\n",
    "    y_tests.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f364f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf704bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47399a",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5508def",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "models = []\n",
    "\n",
    "for i in range(len(vocab_sizes)):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_sizes[i], EMBEDDING_DIM, input_length = max_lengths[i]))\n",
    "    model.add(LSTM(units = 128, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "\n",
    "    model.add(Dense(num_labels[i], activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(models)):\n",
    "    print(i)\n",
    "    models[i].fit(X_trains[i], y_trains[i], batch_size = 64, validation_split = 0.2, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('AccountCodeModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c209f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('AccountCodeModel.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24652ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracys = []\n",
    "for i in range(len(models)):\n",
    "    score, acc = models[i].evaluate(X_tests[i], y_tests[i], batch_size=64, verbose=2)\n",
    "    accuracys.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed42b9a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7afe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, accuracy in zip(dfs.keys(), accuracys):\n",
    "    print(f'Test accuracy for {key}: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ce766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on the test set\n",
    "# y_prob = model.predict(X_test)\n",
    "# y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "# # Convert the NumPy array to a Python list\n",
    "# predictions_list = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465967dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "keys = list(dfs.keys())[:10]  # Replace with your actual keys\n",
    "\n",
    "# Define a color dictionary for each key\n",
    "color_dict = {'EXPENSES_df': 'red', 'CURRENT ASSETS_df': 'green', 'Trade Creditors_df': 'blue', 'Trade Debtors_df': 'orange', 'DIRECT COST OF PROJECT AND OPERATION_df': 'purple',\n",
    "              'Other Creditors_df': 'brown', 'NON-CURRENT ASSETS_df': 'pink', 'CURRENT LIABILITIES_df': 'gray', 'Cash At Bank_df': 'cyan', 'OTHER INCOME_df': 'lime'}\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Ensure consistent lengths of keys and accuracies\n",
    "min_length = min(len(keys), len(accuracies))\n",
    "keys = keys[:min_length]\n",
    "accuracys = accuracys[:min_length]\n",
    "\n",
    "# Create a custom colormap based on the color dictionary with a default color\n",
    "default_color = 'gray'\n",
    "cmap = ListedColormap([color_dict.get(key, default_color) for key in keys])\n",
    "\n",
    "# Plotting the bar chart with colormap\n",
    "bars = plt.bar(keys, accuracys, color=cmap(range(len(keys))))\n",
    "plt.xlabel('DataFrames')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Test Accuracy for Each DataFrame')\n",
    "\n",
    "# Create a legend with color patches\n",
    "legend_patches = [mpatches.Patch(color=color_dict[key], label=key) for key in keys]\n",
    "plt.legend(handles=legend_patches, title='DataFrame Keys', loc='upper left')\n",
    "\n",
    "plt.ylim(0, 1)  # Set y-axis limit between 0 and 1 for accuracy values\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bec4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
